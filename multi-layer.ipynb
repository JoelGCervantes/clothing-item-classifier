{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4b45a4",
   "metadata": {},
   "source": [
    "# Clothing Image Classifier 2\n",
    "This notbook I build a clothing image classifier using A **fully connected nueral network** wih 2 *fully connected layers* and the FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e1f0d",
   "metadata": {},
   "source": [
    "# Define Transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38f51c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5)) # normalize \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9370648",
   "metadata": {},
   "source": [
    "# Data & Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bd2e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "#datasets \n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', download=True, \n",
    "                                             train=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', download=True,\n",
    "                                            train=False, transform=transform)\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, \n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, \n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fda88d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes \n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c3922e",
   "metadata": {},
   "source": [
    "# Check DataLoader Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c4b0675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandal\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# show img function \n",
    "# PyToch sees images as (Channel, Height, Width), but numpy sees them as (Height, width, Channel)\n",
    "# Function is needed to make he switch \n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unormalize\n",
    "    npimg = img.numpy()     #\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) # transpose the tensor in order to match the expected shape\n",
    "\n",
    "# get image batch from trainset \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show the image\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' | '.join(f'{classes[labels[i]]}' for i in range(len(labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be053bd9",
   "metadata": {},
   "source": [
    "## Create Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35437672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCN2, self).__init__()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(28 * 28, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Flatten \n",
    "        x = x.view(-1, 28 * 28)\n",
    "        \n",
    "        # Run through decision layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b223273",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn2 = FCN2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac06de",
   "metadata": {},
   "source": [
    "## Loss function and Back Prop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2054a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(fcn2.parameters(), lr=0.1, momentum=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca10ceb",
   "metadata": {},
   "source": [
    "## Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae30c83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch   400] loss: 2.523\n",
      "[Epoch 1, Batch   800] loss: 2.510\n",
      "[Epoch 1, Batch  1200] loss: nan\n",
      "[Epoch 1, Batch  1600] loss: nan\n",
      "[Epoch 1, Batch  2000] loss: nan\n",
      "[Epoch 1, Batch  2400] loss: nan\n",
      "[Epoch 1, Batch  2800] loss: nan\n",
      "[Epoch 1, Batch  3200] loss: nan\n",
      "[Epoch 1, Batch  3600] loss: nan\n",
      "[Epoch 1, Batch  4000] loss: nan\n",
      "[Epoch 1, Batch  4400] loss: nan\n",
      "[Epoch 1, Batch  4800] loss: nan\n",
      "[Epoch 1, Batch  5200] loss: nan\n",
      "[Epoch 1, Batch  5600] loss: nan\n",
      "[Epoch 1, Batch  6000] loss: nan\n",
      "[Epoch 1, Batch  6400] loss: nan\n",
      "[Epoch 1, Batch  6800] loss: nan\n",
      "[Epoch 1, Batch  7200] loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x109443f70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joelgc/2026/deeplearn440/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/joelgc/2026/deeplearn440/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1628, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "  File \"/Users/joelgc/2026/deeplearn440/env/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 85963) is killed by signal: Abort trap: 6. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfcn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/2026/deeplearn440/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2026/deeplearn440/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[49], line 20\u001b[0m, in \u001b[0;36mFCN2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Run through decision layers\u001b[39;00m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m---> 20\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/2026/deeplearn440/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2026/deeplearn440/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/2026/deeplearn440/env/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    print_interval = 400\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        outputs = fcn2(inputs)\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # back prop\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # stats \n",
    "        running_loss += loss.item()\n",
    "        if i % print_interval == print_interval - 1:\n",
    "            avg_loss = running_loss / print_interval\n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1:5d}] loss: {avg_loss:.3f}')\n",
    "            running_loss = 0.0 \n",
    "print('finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603feed3",
   "metadata": {},
   "source": [
    "## Test Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66bbe934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on 10000 test images: 66.7%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        \n",
    "        images, labels = data\n",
    "        \n",
    "        # raw model scores\n",
    "        outputs = fcn2(images)\n",
    "        # extract highest score\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # count total images processes so far\n",
    "        total += labels.size(0)\n",
    "        # count how many in this batch matches the labels\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total \n",
    "print(f'Final accuracy on {total} test images: {accuracy:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
