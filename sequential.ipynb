{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e481fb7f",
   "metadata": {},
   "source": [
    "# Sequential Clothing Image Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05d2bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a72fa30",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # to tensor\n",
    "    transforms.Normalize((0.5), (0.5)) # normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba000b",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9292379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "trainset = torchvision.datasets.FashionMNIST(train=True, download=True, \n",
    "                                  transform=transform, root='./data')\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(train=False, download=True, \n",
    "                                            transform=transform, root='./data')\n",
    "\n",
    "# data loaders\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=30, \n",
    "#                                          shuffle=True, num_workers=2)\n",
    "#testloader = torch.utils.data.DataLoader(testset, batch_size=30, \n",
    "#                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3a92a",
   "metadata": {},
   "source": [
    "# Check DataLoader Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf109bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2, 0)))\n",
    "    \n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(f'Batch shape: {images.shape}')\n",
    "\n",
    "#images and labels\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' | '.join(f'{classes[labels[j]]}' for j in range(len(labels))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f5103",
   "metadata": {},
   "source": [
    "# Create model from torch sequential model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(), # Turn [30, 1, 28, 28] into [30, 784]\n",
    "    nn.Linear(28 * 28, 1024), # First FC layer\n",
    "    nn.ReLU(), # Activation \n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 10) # Output layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ef0cb",
   "metadata": {},
   "source": [
    "# Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5c46b",
   "metadata": {},
   "source": [
    "# Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cae209",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(30):\n",
    "    running_loss = 0\n",
    "    print_interval = 400\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # forward\n",
    "        outputs = model(inputs)    \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #back\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print loss\n",
    "        running_loss += loss.item()\n",
    "        if i % print_interval ==  print_interval - 1:\n",
    "            avg_loss = running_loss / print_interval\n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1:5d}] loss: {avg_loss:.3f}')\n",
    "            running_loss = 0.0 \n",
    "\n",
    "print('finished training')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0af44",
   "metadata": {},
   "source": [
    "# test Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485c9ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "\n",
    "        images, labels = data\n",
    "\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1) # index with highest score\n",
    "        total += labels.size(0) # count total images procoesses so far\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total \n",
    "print(f'Final Accuracy on {total} test images: {accuracy:.1f}%') \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(activation_type):\n",
    "    act = nn.ReLU() if activation_type == 'ReLU' else nn.Sigmoid()\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(784, 1024),\n",
    "        act,\n",
    "        nn.Linear(1024, 1024),\n",
    "        act,\n",
    "        nn.Linear(1024, 10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b7a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4, Batch 11600] loss: 0.315\n",
      "[Epoch 4, Batch 12000] loss: 0.358\n",
      "[Epoch 4, Batch 12400] loss: 0.308\n",
      "[Epoch 4, Batch 12800] loss: 0.302\n",
      "[Epoch 4, Batch 13200] loss: 0.354\n",
      "[Epoch 4, Batch 13600] loss: 0.327\n",
      "[Epoch 4, Batch 14000] loss: 0.236\n",
      "[Epoch 4, Batch 14400] loss: 0.343\n",
      "[Epoch 4, Batch 14800] loss: 0.316\n",
      "[Epoch 4, Batch 15200] loss: 0.331\n",
      "[Epoch 4, Batch 15600] loss: 0.280\n",
      "[Epoch 4, Batch 16000] loss: 0.317\n",
      "[Epoch 4, Batch 16400] loss: 0.329\n",
      "[Epoch 4, Batch 16800] loss: 0.350\n",
      "[Epoch 4, Batch 17200] loss: 0.326\n",
      "[Epoch 4, Batch 17600] loss: 0.346\n",
      "[Epoch 4, Batch 18000] loss: 0.255\n",
      "[Epoch 4, Batch 18400] loss: 0.365\n",
      "[Epoch 4, Batch 18800] loss: 0.339\n",
      "[Epoch 4, Batch 19200] loss: 0.257\n",
      "[Epoch 4, Batch 19600] loss: 0.330\n",
      "[Epoch 4, Batch 20000] loss: 0.230\n",
      "[Epoch 4, Batch 20400] loss: 0.279\n",
      "[Epoch 4, Batch 20800] loss: 0.293\n",
      "[Epoch 4, Batch 21200] loss: 0.272\n",
      "[Epoch 4, Batch 21600] loss: 0.307\n",
      "[Epoch 4, Batch 22000] loss: 0.340\n",
      "[Epoch 4, Batch 22400] loss: 0.310\n",
      "[Epoch 4, Batch 22800] loss: 0.292\n",
      "[Epoch 4, Batch 23200] loss: 0.286\n",
      "[Epoch 4, Batch 23600] loss: 0.296\n",
      "[Epoch 4, Batch 24000] loss: 0.387\n",
      "[Epoch 4, Batch 24400] loss: 0.367\n",
      "[Epoch 4, Batch 24800] loss: 0.324\n",
      "[Epoch 4, Batch 25200] loss: 0.304\n",
      "[Epoch 4, Batch 25600] loss: 0.247\n",
      "[Epoch 4, Batch 26000] loss: 0.278\n",
      "[Epoch 4, Batch 26400] loss: 0.313\n",
      "[Epoch 4, Batch 26800] loss: 0.365\n",
      "[Epoch 4, Batch 27200] loss: 0.358\n",
      "[Epoch 4, Batch 27600] loss: 0.330\n",
      "[Epoch 4, Batch 28000] loss: 0.351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1069dfca0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joelgc/2026/deeplearn440/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/joelgc/2026/deeplearn440/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1628, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "  File \"/Users/joelgc/2026/deeplearn440/env/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 83372) is killed by signal: Abort trap: 6. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#back\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# print loss\u001b[39;00m\n",
      "File \u001b[0;32m~/2026/deeplearn440/env/lib/python3.9/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2026/deeplearn440/env/lib/python3.9/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2026/deeplearn440/env/lib/python3.9/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_sizes = [1, 10, 1000]\n",
    "learning_rates = [1.0, 0.1, 0.01, 0.001]\n",
    "activations = ['ReLU', 'Sigmoid']\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "results = []\n",
    "\n",
    "for act in activations:\n",
    "    for bs in batch_sizes:\n",
    "        for lr in learning_rates:\n",
    "            # 1. Create DataLoader with batch_size\n",
    "            trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, \n",
    "                                                      shuffle=True, num_workers=2)\n",
    "            testloader = torch.utils.data.DataLoader(testset, batch_size=bs, \n",
    "                                                     shuffle=False, num_workers=2)\n",
    "             \n",
    "            # 2. Initialize Model with activation\n",
    "            model = create_model(act)\n",
    "            \n",
    "            # 3. Initialize Optimizer with lr\n",
    "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "            # 4. Train for X epochs\n",
    "            for epoch in range(30):\n",
    "                running_loss = 0\n",
    "                print_interval = max(1, (60000 // bs) // 5)\n",
    "                for i, data in enumerate(trainloader, 0):\n",
    "                    inputs, labels = data\n",
    "        \n",
    "                    optimizer.zero_grad() \n",
    "        \n",
    "                     # forward\n",
    "                    outputs = model(inputs)    \n",
    "                    loss = criterion(outputs, labels)\n",
    "        \n",
    "                    #back\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "        \n",
    "                    # print loss\n",
    "                    running_loss += loss.item()\n",
    "                    if i % print_interval ==  print_interval - 1:\n",
    "                        avg_loss = running_loss / print_interval\n",
    "                        print(f'[Epoch {epoch + 1}, Batch {i + 1:5d}] loss: {avg_loss:.3f}')\n",
    "                        running_loss = 0.0 \n",
    "\n",
    "            print('finished training')\n",
    "            \n",
    "            # 5. Record final Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for data in testloader:\n",
    "                    images, labels = data\n",
    "                    outputs = model(images)\n",
    "        \n",
    "                    _, predicted = torch.max(outputs.data, 1) # index with highest score\n",
    "                    total += labels.size(0) # count total images procoesses so far\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = 100 * correct / total \n",
    "            results.append({\n",
    "                'Activation': act,\n",
    "                'Batch Size': bs,\n",
    "                'Learning Rate': lr,\n",
    "                'Accuracy': accuracy\n",
    "            })\n",
    "\n",
    "\n",
    "print(\"\\n--- FINAL RESULTS TABLE ---\")\n",
    "print(f\"{'Act':<10} | {'BS':<6} | {'LR':<6} | {'Accuracy':<10}\")\n",
    "for r in results:\n",
    "    print(f\"{r['Activation']:<10} | {r['Batch Size']:<6} | {r['Learning Rate']:<6} | {r['Accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f85c06",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
