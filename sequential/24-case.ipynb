{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJy80ZfLDtfI",
        "outputId": "4b56bcac-0d6d-47d1-a6be-d074a9e28687"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "23vN2fyR__Ee"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qOBZ7nU-AG4O"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # to tensor\n",
        "    transforms.Normalize((0.5), (0.5)) # normalize\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4sdtqh-AMgD",
        "outputId": "c8bfc9f0-2e6c-4dd1-b7c4-cc5aa412716d"
      },
      "outputs": [],
      "source": [
        "# Datasets\n",
        "trainset = torchvision.datasets.FashionMNIST(train=True, download=True,\n",
        "                                  transform=transform, root='../data')\n",
        "\n",
        "testset = torchvision.datasets.FashionMNIST(train=False, download=True,\n",
        "                                            transform=transform, root='../data')\n",
        "\n",
        "classes = classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_48eAiPrBUnw"
      },
      "outputs": [],
      "source": [
        "def create_model(activation_type):\n",
        "    act = nn.ReLU() if activation_type == 'ReLU' else nn.Sigmoid()\n",
        "\n",
        "    return nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(784, 1024),\n",
        "        act,\n",
        "        nn.Linear(1024, 1024),\n",
        "        act,\n",
        "        nn.Linear(1024, 10)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJe1G93v_HEZ",
        "outputId": "619fac47-ed16-4b88-a787-407b14b9a31f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on: cpu\n",
            "Model on GPU: False\n",
            "Activation: ReLU, Batch Size: 1, Learning Rate: 0.1\n",
            "Inputs on GPU: False\n",
            "[Epoch 1, Batch 12000] loss: nan\n",
            "[Epoch 1, Batch 24000] loss: nan\n",
            "[Epoch 1, Batch 36000] loss: nan\n",
            "[Epoch 1, Batch 48000] loss: nan\n",
            "[Epoch 1, Batch 60000] loss: nan\n",
            "[Epoch 2, Batch 12000] loss: nan\n",
            "[Epoch 2, Batch 24000] loss: nan\n",
            "[Epoch 2, Batch 36000] loss: nan\n",
            "[Epoch 2, Batch 48000] loss: nan\n",
            "[Epoch 2, Batch 60000] loss: nan\n",
            "[Epoch 3, Batch 12000] loss: nan\n",
            "[Epoch 3, Batch 24000] loss: nan\n",
            "[Epoch 3, Batch 36000] loss: nan\n",
            "[Epoch 3, Batch 48000] loss: nan\n",
            "[Epoch 3, Batch 60000] loss: nan\n",
            "[Epoch 4, Batch 12000] loss: nan\n",
            "[Epoch 4, Batch 24000] loss: nan\n",
            "[Epoch 4, Batch 36000] loss: nan\n",
            "[Epoch 4, Batch 48000] loss: nan\n",
            "[Epoch 4, Batch 60000] loss: nan\n",
            "[Epoch 5, Batch 12000] loss: nan\n",
            "[Epoch 5, Batch 24000] loss: nan\n",
            "[Epoch 5, Batch 36000] loss: nan\n",
            "[Epoch 5, Batch 48000] loss: nan\n",
            "[Epoch 5, Batch 60000] loss: nan\n",
            "[Epoch 6, Batch 12000] loss: nan\n",
            "[Epoch 6, Batch 24000] loss: nan\n",
            "[Epoch 6, Batch 36000] loss: nan\n",
            "[Epoch 6, Batch 48000] loss: nan\n",
            "[Epoch 6, Batch 60000] loss: nan\n",
            "[Epoch 7, Batch 12000] loss: nan\n",
            "[Epoch 7, Batch 24000] loss: nan\n",
            "[Epoch 7, Batch 36000] loss: nan\n",
            "[Epoch 7, Batch 48000] loss: nan\n",
            "[Epoch 7, Batch 60000] loss: nan\n",
            "[Epoch 8, Batch 12000] loss: nan\n",
            "[Epoch 8, Batch 24000] loss: nan\n",
            "[Epoch 8, Batch 36000] loss: nan\n",
            "[Epoch 8, Batch 48000] loss: nan\n",
            "[Epoch 8, Batch 60000] loss: nan\n",
            "[Epoch 9, Batch 12000] loss: nan\n",
            "[Epoch 9, Batch 24000] loss: nan\n",
            "[Epoch 9, Batch 36000] loss: nan\n",
            "[Epoch 9, Batch 48000] loss: nan\n",
            "[Epoch 9, Batch 60000] loss: nan\n",
            "[Epoch 10, Batch 12000] loss: nan\n",
            "[Epoch 10, Batch 24000] loss: nan\n",
            "[Epoch 10, Batch 36000] loss: nan\n",
            "[Epoch 10, Batch 48000] loss: nan\n",
            "[Epoch 10, Batch 60000] loss: nan\n",
            "[Epoch 11, Batch 12000] loss: nan\n",
            "[Epoch 11, Batch 24000] loss: nan\n",
            "[Epoch 11, Batch 36000] loss: nan\n",
            "[Epoch 11, Batch 48000] loss: nan\n",
            "[Epoch 11, Batch 60000] loss: nan\n",
            "[Epoch 12, Batch 12000] loss: nan\n",
            "[Epoch 12, Batch 24000] loss: nan\n",
            "[Epoch 12, Batch 36000] loss: nan\n",
            "[Epoch 12, Batch 48000] loss: nan\n",
            "[Epoch 12, Batch 60000] loss: nan\n",
            "[Epoch 13, Batch 12000] loss: nan\n",
            "[Epoch 13, Batch 24000] loss: nan\n",
            "[Epoch 13, Batch 36000] loss: nan\n",
            "[Epoch 13, Batch 48000] loss: nan\n",
            "[Epoch 13, Batch 60000] loss: nan\n",
            "[Epoch 14, Batch 12000] loss: nan\n",
            "[Epoch 14, Batch 24000] loss: nan\n",
            "[Epoch 14, Batch 36000] loss: nan\n",
            "[Epoch 14, Batch 48000] loss: nan\n",
            "[Epoch 14, Batch 60000] loss: nan\n",
            "[Epoch 15, Batch 12000] loss: nan\n",
            "[Epoch 15, Batch 24000] loss: nan\n",
            "[Epoch 15, Batch 36000] loss: nan\n",
            "[Epoch 15, Batch 48000] loss: nan\n",
            "[Epoch 15, Batch 60000] loss: nan\n",
            "[Epoch 16, Batch 12000] loss: nan\n",
            "[Epoch 16, Batch 24000] loss: nan\n",
            "[Epoch 16, Batch 36000] loss: nan\n",
            "[Epoch 16, Batch 48000] loss: nan\n",
            "[Epoch 16, Batch 60000] loss: nan\n",
            "[Epoch 17, Batch 12000] loss: nan\n",
            "[Epoch 17, Batch 24000] loss: nan\n",
            "[Epoch 17, Batch 36000] loss: nan\n",
            "[Epoch 17, Batch 48000] loss: nan\n",
            "[Epoch 17, Batch 60000] loss: nan\n",
            "[Epoch 18, Batch 12000] loss: nan\n",
            "[Epoch 18, Batch 24000] loss: nan\n",
            "[Epoch 18, Batch 36000] loss: nan\n",
            "[Epoch 18, Batch 48000] loss: nan\n",
            "[Epoch 18, Batch 60000] loss: nan\n",
            "[Epoch 19, Batch 12000] loss: nan\n",
            "[Epoch 19, Batch 24000] loss: nan\n",
            "[Epoch 19, Batch 36000] loss: nan\n",
            "[Epoch 19, Batch 48000] loss: nan\n",
            "[Epoch 19, Batch 60000] loss: nan\n",
            "[Epoch 20, Batch 12000] loss: nan\n",
            "[Epoch 20, Batch 24000] loss: nan\n",
            "[Epoch 20, Batch 36000] loss: nan\n",
            "[Epoch 20, Batch 48000] loss: nan\n",
            "[Epoch 20, Batch 60000] loss: nan\n",
            "[Epoch 21, Batch 12000] loss: nan\n",
            "[Epoch 21, Batch 24000] loss: nan\n",
            "[Epoch 21, Batch 36000] loss: nan\n",
            "[Epoch 21, Batch 48000] loss: nan\n",
            "[Epoch 21, Batch 60000] loss: nan\n",
            "[Epoch 22, Batch 12000] loss: nan\n",
            "[Epoch 22, Batch 24000] loss: nan\n",
            "[Epoch 22, Batch 36000] loss: nan\n",
            "[Epoch 22, Batch 48000] loss: nan\n",
            "[Epoch 22, Batch 60000] loss: nan\n",
            "[Epoch 23, Batch 12000] loss: nan\n",
            "[Epoch 23, Batch 24000] loss: nan\n",
            "[Epoch 23, Batch 36000] loss: nan\n",
            "[Epoch 23, Batch 48000] loss: nan\n",
            "[Epoch 23, Batch 60000] loss: nan\n",
            "[Epoch 24, Batch 12000] loss: nan\n",
            "[Epoch 24, Batch 24000] loss: nan\n",
            "[Epoch 24, Batch 36000] loss: nan\n",
            "[Epoch 24, Batch 48000] loss: nan\n",
            "[Epoch 24, Batch 60000] loss: nan\n",
            "[Epoch 25, Batch 12000] loss: nan\n",
            "[Epoch 25, Batch 24000] loss: nan\n",
            "[Epoch 25, Batch 36000] loss: nan\n",
            "[Epoch 25, Batch 48000] loss: nan\n",
            "[Epoch 25, Batch 60000] loss: nan\n",
            "[Epoch 26, Batch 12000] loss: nan\n",
            "[Epoch 26, Batch 24000] loss: nan\n",
            "[Epoch 26, Batch 36000] loss: nan\n",
            "[Epoch 26, Batch 48000] loss: nan\n",
            "[Epoch 26, Batch 60000] loss: nan\n",
            "[Epoch 27, Batch 12000] loss: nan\n",
            "[Epoch 27, Batch 24000] loss: nan\n",
            "[Epoch 27, Batch 36000] loss: nan\n",
            "[Epoch 27, Batch 48000] loss: nan\n",
            "[Epoch 27, Batch 60000] loss: nan\n",
            "[Epoch 28, Batch 12000] loss: nan\n",
            "[Epoch 28, Batch 24000] loss: nan\n",
            "[Epoch 28, Batch 36000] loss: nan\n",
            "[Epoch 28, Batch 48000] loss: nan\n",
            "[Epoch 28, Batch 60000] loss: nan\n",
            "[Epoch 29, Batch 12000] loss: nan\n",
            "[Epoch 29, Batch 24000] loss: nan\n",
            "[Epoch 29, Batch 36000] loss: nan\n",
            "[Epoch 29, Batch 48000] loss: nan\n",
            "[Epoch 29, Batch 60000] loss: nan\n",
            "[Epoch 30, Batch 12000] loss: nan\n",
            "[Epoch 30, Batch 24000] loss: nan\n",
            "[Epoch 30, Batch 36000] loss: nan\n",
            "[Epoch 30, Batch 48000] loss: nan\n",
            "[Epoch 30, Batch 60000] loss: nan\n",
            "finished training\n",
            "Model on GPU: False\n",
            "Activation: ReLU, Batch Size: 1, Learning Rate: 0.01\n",
            "Inputs on GPU: False\n",
            "[Epoch 1, Batch 12000] loss: 0.669\n",
            "[Epoch 1, Batch 24000] loss: 0.509\n",
            "[Epoch 1, Batch 36000] loss: 0.466\n",
            "[Epoch 1, Batch 48000] loss: 0.461\n",
            "[Epoch 1, Batch 60000] loss: 0.432\n",
            "[Epoch 2, Batch 12000] loss: 0.404\n",
            "[Epoch 2, Batch 24000] loss: 0.391\n",
            "[Epoch 2, Batch 36000] loss: 0.405\n",
            "[Epoch 2, Batch 48000] loss: 0.382\n",
            "[Epoch 2, Batch 60000] loss: 0.371\n",
            "[Epoch 3, Batch 12000] loss: 0.357\n",
            "[Epoch 3, Batch 24000] loss: 0.352\n",
            "[Epoch 3, Batch 36000] loss: 0.350\n",
            "[Epoch 3, Batch 48000] loss: 0.354\n",
            "[Epoch 3, Batch 60000] loss: 0.339\n",
            "[Epoch 4, Batch 12000] loss: 0.316\n",
            "[Epoch 4, Batch 24000] loss: 0.328\n",
            "[Epoch 4, Batch 36000] loss: 0.322\n",
            "[Epoch 4, Batch 48000] loss: 0.326\n",
            "[Epoch 4, Batch 60000] loss: 0.324\n",
            "[Epoch 5, Batch 12000] loss: 0.304\n",
            "[Epoch 5, Batch 24000] loss: 0.282\n",
            "[Epoch 5, Batch 36000] loss: 0.304\n",
            "[Epoch 5, Batch 48000] loss: 0.308\n",
            "[Epoch 5, Batch 60000] loss: 0.322\n",
            "[Epoch 6, Batch 12000] loss: 0.279\n",
            "[Epoch 6, Batch 24000] loss: 0.286\n",
            "[Epoch 6, Batch 36000] loss: 0.289\n",
            "[Epoch 6, Batch 48000] loss: 0.293\n",
            "[Epoch 6, Batch 60000] loss: 0.288\n",
            "[Epoch 7, Batch 12000] loss: 0.259\n",
            "[Epoch 7, Batch 24000] loss: 0.265\n",
            "[Epoch 7, Batch 36000] loss: 0.274\n",
            "[Epoch 7, Batch 48000] loss: 0.283\n",
            "[Epoch 7, Batch 60000] loss: 0.278\n",
            "[Epoch 8, Batch 12000] loss: 0.257\n",
            "[Epoch 8, Batch 24000] loss: 0.253\n",
            "[Epoch 8, Batch 36000] loss: 0.261\n",
            "[Epoch 8, Batch 48000] loss: 0.252\n",
            "[Epoch 8, Batch 60000] loss: 0.264\n",
            "[Epoch 9, Batch 12000] loss: 0.232\n",
            "[Epoch 9, Batch 24000] loss: 0.235\n",
            "[Epoch 9, Batch 36000] loss: 0.242\n",
            "[Epoch 9, Batch 48000] loss: 0.244\n",
            "[Epoch 9, Batch 60000] loss: 0.252\n",
            "[Epoch 10, Batch 12000] loss: 0.235\n",
            "[Epoch 10, Batch 24000] loss: 0.217\n",
            "[Epoch 10, Batch 36000] loss: 0.240\n",
            "[Epoch 10, Batch 48000] loss: 0.243\n",
            "[Epoch 10, Batch 60000] loss: 0.226\n",
            "[Epoch 11, Batch 12000] loss: 0.218\n",
            "[Epoch 11, Batch 24000] loss: 0.211\n",
            "[Epoch 11, Batch 36000] loss: 0.218\n",
            "[Epoch 11, Batch 48000] loss: 0.222\n",
            "[Epoch 11, Batch 60000] loss: 0.240\n",
            "[Epoch 12, Batch 12000] loss: 0.198\n",
            "[Epoch 12, Batch 24000] loss: 0.217\n",
            "[Epoch 12, Batch 36000] loss: 0.214\n",
            "[Epoch 12, Batch 48000] loss: 0.217\n",
            "[Epoch 12, Batch 60000] loss: 0.223\n",
            "[Epoch 13, Batch 12000] loss: 0.199\n",
            "[Epoch 13, Batch 24000] loss: 0.208\n",
            "[Epoch 13, Batch 36000] loss: 0.205\n",
            "[Epoch 13, Batch 48000] loss: 0.217\n",
            "[Epoch 13, Batch 60000] loss: 0.199\n",
            "[Epoch 14, Batch 12000] loss: 0.177\n",
            "[Epoch 14, Batch 24000] loss: 0.197\n",
            "[Epoch 14, Batch 36000] loss: 0.198\n",
            "[Epoch 14, Batch 48000] loss: 0.197\n",
            "[Epoch 14, Batch 60000] loss: 0.208\n",
            "[Epoch 15, Batch 12000] loss: 0.175\n",
            "[Epoch 15, Batch 24000] loss: 0.192\n",
            "[Epoch 15, Batch 36000] loss: 0.192\n",
            "[Epoch 15, Batch 48000] loss: 0.192\n",
            "[Epoch 15, Batch 60000] loss: 0.202\n",
            "[Epoch 16, Batch 12000] loss: 0.180\n",
            "[Epoch 16, Batch 24000] loss: 0.171\n",
            "[Epoch 16, Batch 36000] loss: 0.191\n",
            "[Epoch 16, Batch 48000] loss: 0.182\n",
            "[Epoch 16, Batch 60000] loss: 0.185\n",
            "[Epoch 17, Batch 12000] loss: 0.169\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
            "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1115ffd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/joelgc/2026/deeplearn440/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/Users/joelgc/2026/deeplearn440/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1628, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/selectors.py\", line 426, in select\n",
            "    key = self._key_from_fd(fd)\n",
            "  File \"/Users/joelgc/2026/deeplearn440/env/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
            "    _error_if_any_worker_fails()\n",
            "RuntimeError: DataLoader worker (pid 28755) is killed by signal: Abort trap: 6. \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#back\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print loss\u001b[39;00m\n",
            "File \u001b[0;32m~/2026/deeplearn440/env/lib/python3.9/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/2026/deeplearn440/env/lib/python3.9/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/2026/deeplearn440/env/lib/python3.9/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "batch_sizes = [1, 10, 1000]\n",
        "learning_rates = [0.1, 0.01, 0.001]\n",
        "activations = ['ReLU', 'Sigmoid']\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "results = []\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on: {device}\")\n",
        "\n",
        "for act in activations:\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            # 1. Create DataLoader with batch_size\n",
        "            trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs,\n",
        "                                                      shuffle=True, num_workers=2)\n",
        "            testloader = torch.utils.data.DataLoader(testset, batch_size=bs,\n",
        "                                                     shuffle=False, num_workers=2)\n",
        "\n",
        "            # 2. Initialize Model with activation\n",
        "            model = create_model(act)\n",
        "            model.to(device)\n",
        "            print(f\"Model on GPU: {next(model.parameters()).is_cuda}\") # Check if model is on GPU\n",
        "\n",
        "            # 3. Initialize Optimizer with lr\n",
        "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
        "\n",
        "            # print settings\n",
        "            print(f\"Activation: {act}, Batch Size: {bs}, Learning Rate: {lr}\")\n",
        "\n",
        "            # 4. Train for X epochs\n",
        "            for epoch in range(30):\n",
        "                running_loss = 0\n",
        "                print_interval = max(1, (60000 // bs) // 5)\n",
        "                for i, data in enumerate(trainloader, 0):\n",
        "                    inputs, labels = data\n",
        "\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    if i == 0 and epoch == 0:\n",
        "                        print(f\"Inputs on GPU: {inputs.is_cuda}\") # Check if inputs are on GPU (first batch only)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                     # forward\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    #back\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # print loss\n",
        "                    running_loss += loss.item()\n",
        "                    if i % print_interval ==  print_interval - 1:\n",
        "                        avg_loss = running_loss / print_interval\n",
        "                        print(f'[Epoch {epoch + 1}, Batch {i + 1:5d}] loss: {avg_loss:.3f}')\n",
        "                        running_loss = 0.0\n",
        "\n",
        "            print('finished training')\n",
        "\n",
        "            # 5. Record final Accuracy\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for data in testloader:\n",
        "                    images, labels = data\n",
        "\n",
        "                    images = images.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    outputs = model(images)\n",
        "\n",
        "                    _, predicted = torch.max(outputs.data, 1) # index with highest score\n",
        "                    total += labels.size(0) # count total images procoeses so far\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            results.append({\n",
        "                'Activation': act,\n",
        "                'Batch Size': bs,\n",
        "                'Learning Rate': lr,\n",
        "                'Accuracy': accuracy\n",
        "            })\n",
        "\n",
        "\n",
        "print(\"\\n--- FINAL RESULTS TABLE ---\")\n",
        "print(f\"{'Act':<10} | {'BS':<6} | {'LR':<6} | {'Accuracy':<10}\")\n",
        "for r in results:\n",
        "    print(f\"{r['Activation']:<10} | {r['Batch Size']:<6} | {r['Learning Rate']:<6} | {r['Accuracy']:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
